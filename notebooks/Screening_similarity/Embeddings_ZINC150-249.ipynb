{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "zinc_data_cleaned = pd.read_csv('zinc_data_100to149_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1086866, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zinc_data_cleaned.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "zinc_data_cleaned = zinc_data_cleaned.drop_duplicates(subset=['smiles'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "zinc_data_cleaned = zinc_data_cleaned.drop_duplicates(subset=['zinc_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1086836, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zinc_data_cleaned.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "zinc_data = zinc_data_cleaned.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>smiles</th>\n",
       "      <th>zinc_id</th>\n",
       "      <th>sanitized_smiles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>OC[C@@H]1CCCNC1</td>\n",
       "      <td>388342</td>\n",
       "      <td>OC[C@@H]1CCCNC1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C[C@@H](O)C(CO)[C@@H](C)O</td>\n",
       "      <td>410291</td>\n",
       "      <td>C[C@@H](O)C(CO)[C@@H](C)O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NCC1(O)CC1</td>\n",
       "      <td>2540025</td>\n",
       "      <td>NCC1(O)CC1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CN1CCN=C1N</td>\n",
       "      <td>3075393</td>\n",
       "      <td>CN1CCN=C1N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>O=Cc1cn(CC(=O)O)cn1</td>\n",
       "      <td>59724508</td>\n",
       "      <td>O=Cc1cn(CC(=O)O)cn1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      smiles   zinc_id           sanitized_smiles\n",
       "0            OC[C@@H]1CCCNC1    388342            OC[C@@H]1CCCNC1\n",
       "1  C[C@@H](O)C(CO)[C@@H](C)O    410291  C[C@@H](O)C(CO)[C@@H](C)O\n",
       "2                 NCC1(O)CC1   2540025                 NCC1(O)CC1\n",
       "3                 CN1CCN=C1N   3075393                 CN1CCN=C1N\n",
       "4        O=Cc1cn(CC(=O)O)cn1  59724508        O=Cc1cn(CC(=O)O)cn1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zinc_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at seyonec/ChemBERTa-zinc-base-v1 were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "# Load the ChemBERTa tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"seyonec/ChemBERTa-zinc-base-v1\", clean_up_tokenization_spaces=False)\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"seyonec/ChemBERTa-zinc-base-v1\")\n",
    "\n",
    "# Ensure the model is in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Function to compute embeddings\n",
    "def get_chemberta_embeddings(smiles_series):\n",
    "    embeddings = []\n",
    "    \n",
    "    for smiles in smiles_series:\n",
    "        # Tokenize the SMILES string\n",
    "        inputs = tokenizer(smiles, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "        \n",
    "        # Forward pass through the model to get hidden states\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs, output_hidden_states=True)\n",
    "        \n",
    "        # Extract the last hidden state (embedding for each token)\n",
    "        hidden_states = outputs.hidden_states[-1]\n",
    "        \n",
    "        # Average the embeddings for all tokens (this creates a single vector for the molecule)\n",
    "        molecule_embedding = hidden_states.mean(dim=1).squeeze()\n",
    "        \n",
    "        # Convert to numpy array (or keep as tensor if preferred)\n",
    "        embeddings.append(molecule_embedding.cpu().numpy())\n",
    "    \n",
    "    return embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The warning is related to the model architecture mismatch when initializing the ChemBERTa model (AutoModelForMaskedLM) from a pretrained checkpoint. The weights for the pooler layer (roberta.pooler.dense) are part of the model used for tasks such as classification, which is not needed for masked language modeling. Therefore, it's safe to ignore this warning because the pooler weights aren't used for the task you are doing (i.e., generating embeddings for molecules).\n",
    "\n",
    "Why this happens:\n",
    "ChemBERTa was pretrained for masked language modeling (MLM), but you are not using it for that task (you're using it to generate embeddings). Hence, some layers, like the pooler, which are meant for downstream tasks (e.g., classification), are not loaded.\n",
    "When using models for purposes other than the originally intended task, warnings like this are common but harmless.\n",
    "What you can do:\n",
    "Ignore the warning: Since you're only using the model to generate embeddings, and not for the original masked language modeling task, the unused weights won't affect your work.\n",
    "\n",
    "Load the model without MaskedLM if you'd like: If you only need embeddings and not the MLM capability, you could use AutoModel instead of AutoModelForMaskedLM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Load the ChemBERTa tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"seyonec/ChemBERTa-zinc-base-v1\", clean_up_tokenization_spaces=False)\n",
    "model = AutoModel.from_pretrained(\"seyonec/ChemBERTa-zinc-base-v1\")\n",
    "\n",
    "# Ensure the model is in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Function to compute embeddings\n",
    "def get_chemberta_embeddings(smiles_series):\n",
    "    embeddings = []\n",
    "    \n",
    "    for smiles in smiles_series:\n",
    "        # Tokenize the SMILES string\n",
    "        inputs = tokenizer(smiles, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "        \n",
    "        # Forward pass through the model to get hidden states\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs, output_hidden_states=True)\n",
    "        \n",
    "        # Extract the last hidden state (embedding for each token)\n",
    "        hidden_states = outputs.hidden_states[-1]\n",
    "        \n",
    "        # Average the embeddings for all tokens (this creates a single vector for the molecule)\n",
    "        molecule_embedding = hidden_states.mean(dim=1).squeeze()\n",
    "        \n",
    "        # Convert to numpy array (or keep as tensor if preferred)\n",
    "        embeddings.append(molecule_embedding.cpu().numpy())\n",
    "    \n",
    "    return embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get embeddings for the ZINC molecules\n",
    "zinc_embeddings = get_chemberta_embeddings(zinc_data['sanitized_smiles'])\n",
    "\n",
    "# Add embeddings to the dataframes for future use\n",
    "zinc_data['embedding'] = zinc_embeddings\n",
    "\n",
    "# Now, you can use these embeddings for similarity searches, clustering, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the embeddings as .csv files\n",
    "zinc_data.to_csv('zinc_data100-149_embeddings.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1086836, 4)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zinc_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
